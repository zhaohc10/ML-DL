{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_03 Data Transformation\n",
    "\n",
    "## Import the initial dataset (without expired records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=r'noExpired.xlsx'\n",
    "all=pd.read_excel(path).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from diag_1_desc,diag_2_desc,diag_3_desc using  \"bag of words\"\n",
    "### Here, we use DataFrameMapper to vectorize a Pandas dataframe for sklearn usage!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diag_1_desc</th>\n",
       "      <th>diag_2_desc</th>\n",
       "      <th>diag_3_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spinal stenosis in cervical region</td>\n",
       "      <td>Spinal stenosis in cervical region</td>\n",
       "      <td>Effusion of joint, site unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First-degree perineal laceration, unspecified ...</td>\n",
       "      <td>Diabetes mellitus of mother, complicating preg...</td>\n",
       "      <td>Sideroblastic anemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cellulitis and abscess of face</td>\n",
       "      <td>Streptococcus infection in conditions classifi...</td>\n",
       "      <td>Diabetes mellitus without mention of complicat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         diag_1_desc  \\\n",
       "0                 Spinal stenosis in cervical region   \n",
       "1  First-degree perineal laceration, unspecified ...   \n",
       "3                     Cellulitis and abscess of face   \n",
       "\n",
       "                                         diag_2_desc  \\\n",
       "0                 Spinal stenosis in cervical region   \n",
       "1  Diabetes mellitus of mother, complicating preg...   \n",
       "3  Streptococcus infection in conditions classifi...   \n",
       "\n",
       "                                         diag_3_desc  \n",
       "0                Effusion of joint, site unspecified  \n",
       "1                               Sideroblastic anemia  \n",
       "3  Diabetes mellitus without mention of complicat...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "\n",
    "cv=CountVectorizer(ngram_range=(1,3),strip_accents='unicode',stop_words='english')\n",
    "hv=HashingVectorizer(stop_words='english',n_features=100)\n",
    "\n",
    "\"\"\"\n",
    "log_cols=['diag_1_desc','diag_2_desc','diag_3_desc']\n",
    "for cols in log_cols:\n",
    "    all[cols]=hv.fit_transform(all[cols].values)\n",
    "#x_log_all = all[ log_cols ].as_matrix()\n",
    "\"\"\"\n",
    "log_cols=['diag_1_desc','diag_2_desc','diag_3_desc']\n",
    "x_log_all = all[ log_cols ]\n",
    "x_log_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7938L, 8L)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from __future__ import division\n",
    "\n",
    "numeric_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "all[ numeric_cols ] = all[ numeric_cols ].astype(float)\n",
    "all[ numeric_cols ]=all[ numeric_cols ].apply(lambda x: MinMaxScaler().fit_transform(x))\n",
    "x_num_all = all[ numeric_cols ].as_matrix()\n",
    "x_num_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the preProcess to the new file\n",
    "writer_noExpired=pd.ExcelWriter('diag_log.xlsx',engine='xlsxwriter')\n",
    "x_log_all.to_excel(writer_noExpired,index=True,sheet_name='diag_log.csv')\n",
    "writer_noExpired.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-50ede7123934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlistT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlistT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcount_vect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'diag_log.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_log_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mxx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "f1=open('diag_log.xlsx','r')\n",
    "listT=[]\n",
    "listT.append(f1)\n",
    "count_vect=CountVectorizer(input='diag_log.xlsx')\n",
    "xx=count_vect.fit_transform(listT)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7938, 100)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc=hv.fit_transform(x_log_all['diag_1_desc'].values.tolist())\n",
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ array([ 0.        ,  0.30357143,  0.66666667,  0.25      ,  0.        ,\n",
       "        0.        ,  0.        ,  1.        ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>],\n",
       "       [ array([ 0.07692308,  0.0625    ,  0.83333333,  0.05      ,  0.        ,\n",
       "        0.        ,  0.        ,  0.5       ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>],\n",
       "       [ array([ 0.23076923,  0.28571429,  0.16666667,  0.05      ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>],\n",
       "       ..., \n",
       "       [ array([ 0.92307692,  0.40178571,  0.33333333,  0.1625    ,  0.        ,\n",
       "        0.        ,  0.1       ,  0.5       ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>],\n",
       "       [ array([ 0.07692308,  0.54464286,  0.16666667,  0.075     ,  0.        ,\n",
       "        0.        ,  0.3       ,  1.        ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>],\n",
       "       [ array([ 0.53846154,  0.53571429,  0.        ,  0.2125    ,  0.        ,\n",
       "        0.        ,  0.1       ,  1.        ]),\n",
       "        <1x100 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>]], dtype=object)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#x_all = np.hstack(( x_num_all,  cc ))\n",
    "#x_all = zip(list(x_num_all),  list(cc ))\n",
    "x_all = zip((x_num_all),  (cc ))\n",
    "x_all=np.array(x_all)\n",
    "x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "# y\n",
    "y_all = all.readmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-482672f5098c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"training started\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlogReg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_all\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"training ended\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0met\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                              % self.C)\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'liblinear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)\u001b[0m\n\u001b[0;32m    442\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    443\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                     ensure_min_features)\n\u001b[0m\u001b[0;32m    445\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logReg=linear_model.LogisticRegression()\n",
    "\n",
    "import time\n",
    "#training\n",
    "st = time.time()\n",
    "print \"training started\"\n",
    "logReg.fit( x_all, y_all )\n",
    "print \"training ended\"\n",
    "et = time.time()\n",
    "tt = et - st\n",
    "print \"Training Time = \" + str(tt) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-310580e7fdd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataFrameMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'diag_1_desc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'diag_2_desc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'diag_3_desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvecDiag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_log_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn_pandas\\__init__.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_col_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \"\"\"\n\u001b[1;32m--> 776\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 804\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 236\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mapper=DataFrameMapper([(['diag_1_desc','diag_2_desc','diag_3_desc'],CountVectorizer(binary=True,ngram_range=(1,2)))]) \n",
    "\n",
    "vecDiag=mapper.fit_transform(x_log_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "1         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "3         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "4         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "5         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "7         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9         (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "10        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "11        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "12        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "13        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "14        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "15        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "16        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "17        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "18        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "20        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "21        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "22        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "23        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "24        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "26        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "27        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "28        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "30        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "31        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "32        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "33        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "34        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "35        (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "                              ...                        \n",
       "9963      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9965      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9966      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9967      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9968      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9969      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9971      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9972      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9975      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9976      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9977      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9978      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9980      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9981      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9982      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9983      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9985      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9987      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9988      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9989      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9990      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9991      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9992      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9993      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9994      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9995      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9996      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9997      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9998      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "9999      (0, 35)\\t-0.5\\n  (0, 41)\\t0.5\\n  (0, 75)\\t0....\n",
       "Name: diag_3_desc, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_log_all['diag_3_desc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert numeric of dataframe to matrix for sklearn usage!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.30357143,  0.66666667, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.07692308,  0.0625    ,  0.83333333, ...,  0.        ,\n",
       "         0.        ,  0.5       ],\n",
       "       [ 0.23076923,  0.28571429,  0.16666667, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.92307692,  0.40178571,  0.33333333, ...,  0.        ,\n",
       "         0.1       ,  0.5       ],\n",
       "       [ 0.07692308,  0.54464286,  0.16666667, ...,  0.        ,\n",
       "         0.3       ,  1.        ],\n",
       "       [ 0.53846154,  0.53571429,  0.        , ...,  0.        ,\n",
       "         0.1       ,  1.        ]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from __future__ import division\n",
    "\n",
    "numeric_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "all[ numeric_cols ] = all[ numeric_cols ].astype(float)\n",
    "all[ numeric_cols ]=all[ numeric_cols ].apply(lambda x: MinMaxScaler().fit_transform(x))\n",
    "x_num_all = all[ numeric_cols ].as_matrix()\n",
    "x_num_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical of dataframe to matrix  using factorize for sklearn usage!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical\n",
    "cat_all = all.drop( numeric_cols + [ 'readmitted']+ log_cols, axis = 1 )\n",
    "\n",
    "fac_x_cat_all = pd.DataFrame()\n",
    "cat_cols = list(cat_all.columns.values)\n",
    "for col in cat_cols:\n",
    "    all_cur, _ = pd.factorize(cat_all[col])\n",
    "    fac_x_cat_all[col] = all_cur\n",
    "\n",
    "fac_x_cat_all = fac_x_cat_all.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all the feature to be trained as x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_all = np.hstack(( x_num_all,  fac_x_cat_all, vecDiag_1,vecDiag_2,vecDiag_3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7938L, 7337L)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_all = all.readmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid fmt: array([0, 0, 0, ..., 1, 0, 0], dtype=int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-0d9044877ba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataTransform.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\AndrewPro\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.pyc\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments)\u001b[0m\n\u001b[0;32m   1071\u001b[0m                 \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid fmt: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid fmt: array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
     ]
    }
   ],
   "source": [
    "np.savetxt('dataTransform.csv',x_all,y_all,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def dataTransform(filepath):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn_pandas import DataFrameMapper\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    all=pd.read_excel(path).dropna()\n",
    "    cv=CountVectorizer(ngram_range=(1,3),strip_accents='unicode',stop_words='english')\n",
    "    hv=HashingVectorizer(stop_words='english',n_features=100)\n",
    "\n",
    "    log_cols=['diag_1_desc','diag_2_desc','diag_3_desc']\n",
    "    x_log_all = all[ log_cols ]\n",
    "\n",
    "    numeric_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "    all[ numeric_cols ] = all[ numeric_cols ].astype(float)\n",
    "    all[ numeric_cols ]=all[ numeric_cols ].apply(lambda x: MinMaxScaler().fit_transform(x))\n",
    "    x_num_all = all[ numeric_cols ].as_matrix()\n",
    "\n",
    "    # categorical\n",
    "    x_cat_all = all.drop( numeric_cols + [ 'readmitted']+ log_cols, axis = 1 )\n",
    "\n",
    "    mapper_1=DataFrameMapper([\n",
    "        (['bagwords_1','bagwords_2','bagwords_3'],None),\n",
    "        ('diag_1_desc',cv)\n",
    "    ]) \n",
    "\n",
    "\n",
    "    mapper_2=DataFrameMapper([\n",
    "        (['bagwords_1','bagwords_2','bagwords_3'],None),\n",
    "        ('diag_2_desc',cv)\n",
    "    ]) \n",
    "\n",
    "\n",
    "    mapper_3=DataFrameMapper([\n",
    "        (['bagwords_1','bagwords_2','bagwords_3'],None),\n",
    "        ('diag_3_desc',cv)\n",
    "    ]) \n",
    "\n",
    "    vecDiag_1=mapper_1.fit_transform(x_log_all)\n",
    "    vecDiag_2=mapper_1.fit_transform(x_log_all)\n",
    "    vecDiag_3=mapper_1.fit_transform(x_log_all)\n",
    "\n",
    "    fac_x_cat_all = pd.DataFrame()\n",
    "    cat_cols = list(x_cat_all.columns.values)\n",
    "    for col in cat_cols:\n",
    "        all_cur, _ = pd.factorize(cat_all[col])\n",
    "        fac_x_cat_all[col] = all_cur\n",
    "\n",
    "    fac_x_cat_all = fac_x_cat_all.as_matrix()\n",
    "\n",
    "    x_all = np.hstack(( x_num_all,  fac_x_cat_all, vecDiag_1,vecDiag_2,vecDiag_3 ))\n",
    "    y_all = all.readmitted\n",
    "\n",
    "    return x_all, y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
